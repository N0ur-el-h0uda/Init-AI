{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":108446,"databundleVersionId":13146456,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force CPU\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nfrom difflib import SequenceMatcher\n\n# ✅ Arabic Char Normalization\ndef normalize_arabic(text):\n    import re\n    text = re.sub('[ًٌٍَُِّْ]', '', text)  # Remove harakat\n    text = re.sub('[إأآا]', 'ا', text)\n    text = re.sub('ى', 'ي', text)\n    text = re.sub('ؤ', 'و', text)\n    text = re.sub('ئ', 'ي', text)\n    text = re.sub('ة', 'ه', text)\n    return text\n\n# ✅ Define Arabic Characters\narabic_chars = list(\"ابتثجحخدذرزسشصضطظعغفقكلمنهوي\")\nextra_tokens = ['<PAD>', '<SOS>', '<EOS>']\nall_chars = extra_tokens + arabic_chars\nchar_to_idx = {char: idx for idx, char in enumerate(all_chars)}\nidx_to_char = {idx: char for char, idx in char_to_idx.items()}\nPAD_IDX = char_to_idx['<PAD>']\n\n# ✅ Config\ndevice = torch.device(\"cpu\")\nBATCH_SIZE = 16\nIMG_HEIGHT = 64\nIMG_WIDTH = 256\nMAX_TEXT_LEN = 32\nEPOCHS = 3\n\n# ✅ Dataset\nclass OCRDataset(Dataset):\n    def __init__(self, df, img_dir, transform=None, is_test=False):\n        self.df = df\n        self.img_dir = img_dir\n        self.transform = transform\n        self.is_test = is_test\n\n    def __len__(self):\n        return len(self.df)\n\n    def encode_text(self, text):\n        text = normalize_arabic(text)\n        encoded = [char_to_idx[c] for c in text if c in char_to_idx]\n        return encoded\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = os.path.join(self.img_dir, row[\"image\"])\n        image = Image.open(img_path).convert(\"L\")\n        if self.transform:\n            image = self.transform(image)\n        else:\n            image = transforms.ToTensor()(image)\n        if not self.is_test:\n            target = self.encode_text(row[\"text\"])\n            return image, torch.tensor(target, dtype=torch.long)\n        else:\n            return image, row[\"image\"]\n\n# ✅ Collate\ndef collate_fn(batch):\n    images, targets = zip(*batch)\n    images = torch.stack(images)\n    lengths = torch.tensor([len(t) for t in targets])\n    targets = torch.cat(targets)\n    return images, targets, lengths\n\n# ✅ Model\nclass SimpleCRNN(nn.Module):\n    def __init__(self, vocab_size):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2),  # (32, 32, 128)\n            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(),\n            nn.MaxPool2d(2),  # (64, 16, 64)\n        )\n        self.rnn = nn.LSTM(64 * 16, 128, num_layers=2, bidirectional=True, batch_first=True)\n        self.fc = nn.Linear(128 * 2, vocab_size)\n\n    def forward(self, x):\n        x = self.cnn(x)  # (B, C, H, W)\n        b, c, h, w = x.size()\n        x = x.permute(0, 3, 1, 2)  # (B, W, C, H)\n        x = x.view(b, w, -1)  # (B, W, C*H)\n        x, _ = self.rnn(x)  # (B, W, 2*H)\n        x = self.fc(x)  # (B, W, vocab)\n        return x.log_softmax(2)\n\n# ✅ CER\ndef cer(s1, s2):\n    return 1 - SequenceMatcher(None, s1, s2).ratio()\n\n# ✅ Decode\ndef decode_prediction(logits):\n    pred_indices = logits.argmax(2).cpu().numpy()\n    texts = []\n    for pred in pred_indices:\n        tokens = []\n        last = -1\n        for p in pred:\n            if p != last and p != PAD_IDX:\n                tokens.append(idx_to_char.get(p, ''))\n            last = p\n        texts.append(\"\".join(tokens))\n    return texts\n\n# ✅ Load Data\ntrain_df = pd.read_csv(\"/kaggle/input/mc-datathon-2025-arabic-manuscripts-digitization/train_df.csv\", encoding=\"utf-8\")\ntest_df = pd.read_csv(\"/kaggle/input/mc-datathon-2025-arabic-manuscripts-digitization/test_df.csv\", encoding=\"utf-8\")\n\n# ✅ Image directories\nTRAIN_IMG_DIR = \"/kaggle/input/mc-datathon-2025-arabic-manuscripts-digitization/train/train\"\nTEST_IMG_DIR = \"/kaggle/input/mc-datathon-2025-arabic-manuscripts-digitization/test/test\"\n\ntransform = transforms.Compose([\n    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n    transforms.ToTensor()\n])\n\n# ✅ Dataloaders\ntrain_dataset = OCRDataset(train_df, TRAIN_IMG_DIR, transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n\ntest_dataset = OCRDataset(test_df, TEST_IMG_DIR, transform, is_test=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n\n# ✅ Model + Optim\nmodel = SimpleCRNN(len(all_chars)).to(device)\ncriterion = nn.CTCLoss(blank=PAD_IDX, zero_infinity=True)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# ✅ Training Loop\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    total_loss = 0\n    for images, targets, lengths in train_loader:\n        images = images.to(device)\n        targets = targets.to(device)\n        logits = model(images)\n        log_probs = logits.permute(1, 0, 2)\n        input_lengths = torch.full((logits.size(0),), log_probs.size(0), dtype=torch.long)\n        loss = criterion(log_probs, targets, input_lengths, lengths)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch}/{EPOCHS} - Loss: {total_loss/len(train_loader):.4f}\")\n\n    # CER Eval\n    model.eval()\n    with torch.no_grad():\n        cers = []\n        for i in range(min(20, len(train_dataset))):\n            image, target = train_dataset[i]\n            image = image.unsqueeze(0).to(device)\n            logits = model(image)\n            decoded = decode_prediction(logits)[0]\n            truth = \"\".join([idx_to_char[i.item()] for i in target])\n            cers.append(cer(decoded, truth))\n        print(f\"Epoch {epoch} CER: {np.mean(cers):.4f}\")\n\n# ✅ Submission\nmodel.eval()\npredictions = []\nwith torch.no_grad():\n    for image, file_name in test_loader:\n        image = image.to(device)\n        logits = model(image)\n        decoded = decode_prediction(logits)[0]\n        predictions.append({\"image\": file_name[0], \"text\": decoded})\n\nsubmission = pd.DataFrame(predictions)\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ Saved submission.csv\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-23T20:54:21.084391Z","iopub.execute_input":"2025-07-23T20:54:21.084814Z","iopub.status.idle":"2025-07-23T21:01:45.143082Z","shell.execute_reply.started":"2025-07-23T20:54:21.084782Z","shell.execute_reply":"2025-07-23T21:01:45.142112Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/3 - Loss: 1.5568\nEpoch 1 CER: 0.9956\nEpoch 2/3 - Loss: 1.3946\nEpoch 2 CER: 0.9862\nEpoch 3/3 - Loss: 1.3262\nEpoch 3 CER: 0.9033\n✅ Saved submission.csv\n","output_type":"stream"}],"execution_count":9}]}