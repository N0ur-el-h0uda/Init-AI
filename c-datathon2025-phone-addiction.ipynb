{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":108444,"databundleVersionId":13146392,"sourceType":"competition"}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.base import clone\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Load data\ntrain_df = pd.read_csv('/kaggle/input/mc-datathon-2025-phone-addiction/train.csv')\ntest_df = pd.read_csv('/kaggle/input/mc-datathon-2025-phone-addiction/test.csv')\n\n# --- Feature Engineering ---\ndef advanced_feature_engineering(df):\n    df = df.copy()\n    df[\"usage_sleep_ratio\"] = df[\"Daily_Usage_Hours\"] / (df[\"Sleep_Hours\"] + 0.1)\n    df[\"weekend_weekday_ratio\"] = df[\"Weekend_Usage_Hours\"] / (df[\"Daily_Usage_Hours\"] + 0.1)\n    df[\"distraction_time\"] = df[\"Time_on_Gaming\"] + df[\"Time_on_Social_Media\"]\n    df[\"focus_ratio\"] = df[\"Time_on_Education\"] / (df[\"distraction_time\"] + 0.1)\n    df[\"total_screen_time\"] = df[\"Time_on_Social_Media\"] + df[\"Time_on_Gaming\"] + df[\"Time_on_Education\"]\n    df[\"mental_health_score\"] = df[\"Anxiety_Level\"] + df[\"Depression_Level\"] - df[\"Self_Esteem\"]\n    df[\"social_academic_balance\"] = df[\"Social_Interactions\"] * df[\"Academic_Performance\"] / 100\n    df[\"usage_per_app\"] = df[\"Daily_Usage_Hours\"] / (df[\"Apps_Used_Daily\"] + 0.1)\n    df[\"checks_per_hour\"] = df[\"Phone_Checks_Per_Day\"] / 24\n    df[\"intensive_usage\"] = (df[\"Phone_Checks_Per_Day\"] > 100).astype(int)\n    df[\"sleep_screen_conflict\"] = df[\"Screen_Time_Before_Bed\"] * df[\"Daily_Usage_Hours\"]\n    df[\"sleep_deficit\"] = np.maximum(0, 8 - df[\"Sleep_Hours\"])\n    df[\"age_usage_interaction\"] = df[\"Age\"] * df[\"Daily_Usage_Hours\"]\n    df[\"teenage_peak\"] = ((df[\"Age\"] >= 15) & (df[\"Age\"] <= 17)).astype(int)\n    df[\"high_anxiety_low_esteem\"] = ((df[\"Anxiety_Level\"] > 7) & (df[\"Self_Esteem\"] < 4)).astype(int)\n    df[\"social_isolation\"] = (df[\"Social_Interactions\"] < 3).astype(int)\n    df[\"wellness_score\"] = df[\"Exercise_Hours\"] + df[\"Sleep_Hours\"] - df[\"Anxiety_Level\"]\n    df[\"lifestyle_balance\"] = (df[\"Exercise_Hours\"] + df[\"Sleep_Hours\"]) / (df[\"Daily_Usage_Hours\"] + 0.1)\n    return df\n\ndef preprocess_data(train_df, test_df):\n    test_ids = test_df.get('id', range(len(test_df)))\n    columns_to_drop = ['id', 'Location', 'Name']\n    train_df = train_df.drop(columns=[c for c in columns_to_drop if c in train_df.columns])\n    test_df = test_df.drop(columns=[c for c in columns_to_drop if c in test_df.columns])\n    \n    if 'School_Grade' in train_df.columns:\n        train_df['Grade_Numeric'] = train_df['School_Grade'].str.extract('(\\d+)').astype(float)\n        test_df['Grade_Numeric'] = test_df['School_Grade'].str.extract('(\\d+)').astype(float)\n        train_df = train_df.drop('School_Grade', axis=1)\n        test_df = test_df.drop('School_Grade', axis=1)\n\n    train_df = advanced_feature_engineering(train_df)\n    test_df = advanced_feature_engineering(test_df)\n\n    categorical_columns = ['Gender', 'Phone_Usage_Purpose']\n    if 'Parental_Control' in train_df.columns:\n        categorical_columns.append('Parental_Control')\n\n    for col in categorical_columns:\n        le = LabelEncoder()\n        train_df[col] = le.fit_transform(train_df[col].astype(str))\n        test_df[col] = test_df[col].astype(str).apply(lambda x: x if x in le.classes_ else le.classes_[0])\n        test_df[col] = le.transform(test_df[col])\n\n    corr_matrix = train_df.corr()\n    target_corr = corr_matrix[\"Addiction_Level\"].abs()\n    important_features = ['usage_sleep_ratio', 'mental_health_score', 'focus_ratio', \n                          'weekend_weekday_ratio', 'social_academic_balance']\n    low_corr_cols = target_corr[target_corr < 0.1].index.tolist()\n    low_corr_cols = [col for col in low_corr_cols if col not in important_features and col != 'Addiction_Level']\n    \n    train_df = train_df.drop(columns=low_corr_cols)\n    test_df = test_df.drop(columns=[c for c in low_corr_cols if c in test_df.columns])\n    \n    return train_df, test_df, test_ids\n\n# Preprocess\ntrain_processed, test_processed, test_ids = preprocess_data(train_df.copy(), test_df.copy())\nX = train_processed.drop(\"Addiction_Level\", axis=1)\ny = train_processed[\"Addiction_Level\"]\ny_log = np.log1p(y)\n\n# Scale\nscaler = RobustScaler()\nX_scaled = scaler.fit_transform(X)\ntest_scaled = scaler.transform(test_processed)\n\n# Select features\nselector = SelectKBest(score_func=f_regression, k=20)\nX_selected = selector.fit_transform(X_scaled, y_log)\ntest_selected = selector.transform(test_scaled)\nfeature_names = np.array(X.columns)[selector.get_support()]\n\n# KFold stacking\ndef generate_meta_features(models, X, y, X_test, n_folds=5):\n    meta_train = np.zeros((X.shape[0], len(models)))\n    meta_test = np.zeros((X_test.shape[0], len(models)))\n    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n    for i, (name, model) in enumerate(models.items()):\n        print(f\"Fitting {name}...\")\n        test_preds = np.zeros((X_test.shape[0], n_folds))\n        for j, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n            model_clone = clone(model)\n            model_clone.fit(X[train_idx], y[train_idx])\n            meta_train[val_idx, i] = model_clone.predict(X[val_idx])\n            test_preds[:, j] = model_clone.predict(X_test)\n        meta_test[:, i] = test_preds.mean(axis=1)\n    return meta_train, meta_test\n\n# Base models\nbase_models = {\n    'xgb': XGBRegressor(n_estimators=600, max_depth=4, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42),\n    'lgb': LGBMRegressor(n_estimators=600, max_depth=4, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, random_state=42),\n    'gb': GradientBoostingRegressor(n_estimators=400, max_depth=4, learning_rate=0.05, subsample=0.8, random_state=42),\n    'cat': CatBoostRegressor(iterations=600, learning_rate=0.03, depth=6, verbose=0, random_seed=42)\n}\n\nmeta_train, meta_test = generate_meta_features(base_models, X_selected, y_log.values, test_selected)\n\n# Meta model\nmeta_model = Ridge(alpha=1.0)\nmeta_model.fit(meta_train, y_log)\nmse = mean_squared_error(y_log, meta_model.predict(meta_train))\nprint(f\"ðŸ“‰ Final stacked model log-MSE: {mse:.6f}\")\n\n# Predict and submit\nfinal_predictions_log = meta_model.predict(meta_test)\nfinal_predictions = np.expm1(final_predictions_log)\nsubmission = pd.DataFrame({'id': test_ids, 'Addiction_Level': final_predictions})\nsubmission.to_csv('submission.csv', index=False)\nprint(\"Submission saved as 'submission.csv'\")\n\n# Feature importance\ncat_model = base_models['cat']\ncat_model.fit(X_selected, y_log)\nimportance_df = pd.DataFrame({\n    'feature': feature_names,\n    'importance': cat_model.get_feature_importance()\n}).sort_values(by='importance', ascending=False)\n\nprint(\"\\nTop 10 CatBoost Features:\")\nprint(importance_df.head(10).to_string(index=False))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}