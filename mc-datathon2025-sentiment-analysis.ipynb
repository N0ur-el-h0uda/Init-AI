{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":108235,"databundleVersionId":13123466,"sourceType":"competition"},{"sourceId":12549523,"sourceType":"datasetVersion","datasetId":7923496}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\nos.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n# Imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, Trainer, TrainingArguments\nfrom transformers import DataCollatorWithPadding\nfrom sklearn.model_selection import train_test_split\nfrom transformers import RobertaForSequenceClassification\n# Check GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using:\", device)\n\n# Load data\ntrain_df = pd.read_csv(\"/kaggle/input/mc-datathon-2025-sentiment-analysis/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/mc-datathon-2025-sentiment-analysis/test.csv\")\n\n# Drop NaN in sentiment\ntrain_df = train_df.dropna(subset=[\"sentiment\"])\n\n# Encode labels (0=negative, 1=neutral, 2=positive)\nlabel_enc = LabelEncoder()\ntrain_df[\"label\"] = label_enc.fit_transform(train_df[\"sentiment\"])\nprint(\"Classes:\", label_enc.classes_)\n\n# Load locally uploaded RoBERTa model\nmodel_path = \"/kaggle/input/roberta-base/roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(model_path)\nmodel = RobertaForSequenceClassification.from_pretrained(model_path, num_labels=3).to(device)\n\n# Dataset class\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels=None):\n        self.encodings = tokenizer(texts, truncation=True, padding=True)\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels is not None:\n            item[\"labels\"] = torch.tensor(self.labels[idx])\n        return item\n\n# Split train/val\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_df[\"text\"].tolist(),\n    train_df[\"label\"].tolist(),\n    test_size=0.1,\n    stratify=train_df[\"label\"],\n    random_state=42\n)\n\ntrain_dataset = SentimentDataset(train_texts, train_labels)\nval_dataset = SentimentDataset(val_texts, val_labels)\n\n# Metric function\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=1)\n    f1 = f1_score(labels, preds, average=\"macro\")\n    return {\"f1\": f1}\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    report_to=\"none\" \n)\n\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=DataCollatorWithPadding(tokenizer),\n    compute_metrics=compute_metrics,\n)\n\n# Train\ntrainer.train()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-23T07:45:21.574668Z","iopub.execute_input":"2025-07-23T07:45:21.574989Z","iopub.status.idle":"2025-07-23T13:48:45.615146Z","shell.execute_reply.started":"2025-07-23T07:45:21.574963Z","shell.execute_reply":"2025-07-23T13:48:45.612768Z"}},"outputs":[{"name":"stderr","text":"2025-07-23 07:45:39.323363: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753256739.548464      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753256739.616004      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Using: cpu\nClasses: ['negative' 'neutral' 'positive']\n","output_type":"stream"},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_36/4013610155.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='430' max='430' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [430/430 6:01:42, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=430, training_loss=0.7578576376271803, metrics={'train_runtime': 21762.7261, 'train_samples_per_second': 0.316, 'train_steps_per_second': 0.02, 'total_flos': 1807062952955904.0, 'train_loss': 0.7578576376271803, 'epoch': 1.0})"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"\n# Predict on test set\ntest_dataset = SentimentDataset(test_df[\"text\"].tolist())\npreds = trainer.predict(test_dataset)\npred_labels = np.argmax(preds.predictions, axis=1)\n\n# Decode labels\ndecoded_preds = label_enc.inverse_transform(pred_labels)\n\n# Submission\nsubmission = pd.DataFrame({\n    \"id\": test_df[\"id\"],\n    \"sentiment\": decoded_preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T13:48:45.619243Z","iopub.execute_input":"2025-07-23T13:48:45.619683Z","iopub.status.idle":"2025-07-23T14:59:39.081924Z","execution_failed":"2025-07-23T16:33:03.652Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}}],"execution_count":null}]}